(function() {
  var __indexOf = [].indexOf || function(item) { for (var i = 0, l = this.length; i < l; i++) { if (i in this && this[i] === item) return i; } return -1; };

  describe('Go grammar', function() {
    var grammar;
    grammar = null;
    beforeEach(function() {
      waitsForPromise(function() {
        return atom.packages.activatePackage('language-go');
      });
      return runs(function() {
        return grammar = atom.syntax.grammarForScopeName('source.go');
      });
    });
    it('parses the grammar', function() {
      expect(grammar).toBeTruthy();
      return expect(grammar.scopeName).toBe('source.go');
    });
    it('tokenizes comments', function() {
      var tokens;
      tokens = grammar.tokenizeLine('// I am a comment').tokens;
      expect(tokens[0].value).toEqual('//');
      expect(tokens[0].scopes).toEqual(['source.go', 'comment.line.double-slash.go', 'punctuation.definition.comment.go']);
      expect(tokens[1].value).toEqual(' I am a comment');
      expect(tokens[1].scopes).toEqual(['source.go', 'comment.line.double-slash.go']);
      tokens = grammar.tokenizeLines('/*\nI am a comment\n*/');
      expect(tokens[0][0].value).toEqual('/*');
      expect(tokens[0][0].scopes).toEqual(['source.go', 'comment.block.go', 'punctuation.definition.comment.go']);
      expect(tokens[1][0].value).toEqual('I am a comment');
      expect(tokens[1][0].scopes).toEqual(['source.go', 'comment.block.go']);
      expect(tokens[2][0].value).toEqual('*/');
      return expect(tokens[2][0].scopes).toEqual(['source.go', 'comment.block.go', 'punctuation.definition.comment.go']);
    });
    it('tokenizes strings', function() {
      var delim, delims, scope, tokens, _results;
      delims = {
        'string.quoted.double.go': '"',
        'string.quoted.single.go': '\'',
        'string.quoted.double.raw.backtick.go': '`'
      };
      _results = [];
      for (scope in delims) {
        delim = delims[scope];
        tokens = grammar.tokenizeLine(delim + 'I am a string' + delim).tokens;
        expect(tokens[0].value).toEqual(delim);
        expect(tokens[0].scopes).toEqual(['source.go', scope, 'punctuation.definition.string.begin.go']);
        expect(tokens[1].value).toEqual('I am a string');
        expect(tokens[1].scopes).toEqual(['source.go', scope]);
        expect(tokens[2].value).toEqual(delim);
        _results.push(expect(tokens[2].scopes).toEqual(['source.go', scope, 'punctuation.definition.string.end.go']));
      }
      return _results;
    });
    it('tokenizes Printf verbs in strings', function() {
      var tokens, verb, verbs, _i, _len, _results;
      verbs = ['%# x', '%-5s', '%5s', '%05s', '%.5s', '%10.1q', '%10v', '%-10v', '%.0d', '%.d', '%+07.2f', '%0100d', '%0.100f', '%#064x', '%+.3F', '%-#20.8x', '%[1]d', '%[2]*[1]d', '%[3]*.[2]*[1]f', '%[3]*.[2]f', '%3.[2]d', '%.[2]d', '%-+[1]x', '%d', '%-d', '%+d', '%#d', '% d', '%0d', '%1.2d', '%-1.2d', '%+1.2d', '%-+1.2d', '%*d', '%.*d', '%*.*d', '%0*d', '%-*d'];
      _results = [];
      for (_i = 0, _len = verbs.length; _i < _len; _i++) {
        verb = verbs[_i];
        tokens = grammar.tokenizeLine('"' + verb + '"').tokens;
        expect(tokens[0].value).toEqual('"', expect(tokens[0].scopes).toEqual(['source.go', 'string.quoted.double.go', 'punctuation.definition.string.begin.go']));
        expect(tokens[1].value).toEqual(verb);
        expect(tokens[1].scopes).toEqual(['source.go', 'string.quoted.double.go', 'constant.escape.format-verb.go']);
        _results.push(expect(tokens[2].value).toEqual('"', expect(tokens[2].scopes).toEqual(['source.go', 'string.quoted.double.go', 'punctuation.definition.string.end.go'])));
      }
      return _results;
    });
    it('tokenizes character escapes in strings', function() {
      var escape, escapes, tokens, _i, _len;
      escapes = ['\\a', '\\b', '\\f', '\\n', '\\r', '\\t', '\\v', '\\\\', '\\000', '\\007', '\\377', '\\x07', '\\xff', '\\u12e4', '\\U00101234'];
      for (_i = 0, _len = escapes.length; _i < _len; _i++) {
        escape = escapes[_i];
        tokens = grammar.tokenizeLine('"' + escape + '"').tokens;
        expect(tokens[1].value).toEqual(escape);
        expect(tokens[1].scopes).toEqual(['source.go', 'string.quoted.double.go', 'constant.character.escape.go']);
      }
      tokens = grammar.tokenizeLine('"\\""').tokens;
      expect(tokens[1].value).toEqual('\\"');
      expect(tokens[1].scopes).toEqual(['source.go', 'string.quoted.double.go', 'constant.character.escape.go']);
      tokens = grammar.tokenizeLine('\'\\\'\'').tokens;
      return expect(tokens[1].value).toEqual('\\\'', expect(tokens[1].scopes).toEqual(['source.go', 'string.quoted.single.go', 'constant.character.escape.go']));
    });
    it('tokenizes invalid whitespace around chan annotations', function() {
      var expr, invalid, invalids, tokens, _results;
      invalids = {
        'chan <- sendonly': ' ',
        '<- chan recvonly': ' ',
        'trailingspace   ': '   ',
        'trailingtab\t': '\t'
      };
      _results = [];
      for (expr in invalids) {
        invalid = invalids[expr];
        tokens = grammar.tokenizeLine(expr).tokens;
        expect(tokens[1].value).toEqual(invalid);
        _results.push(expect(tokens[1].scopes).toEqual(['source.go', 'invalid.illegal.go']));
      }
      return _results;
    });
    it('tokenizes keywords', function() {
      var keyword, keywordLists, list, scope, tokens, _results;
      keywordLists = {
        'keyword.go': ['var', 'const', 'type', 'struct', 'interface', 'case', 'default'],
        'keyword.directive.go': ['package', 'import'],
        'keyword.statement.go': ['defer', 'go', 'goto', 'return', 'break', 'continue', 'fallthrough'],
        'keyword.conditional.go': ['if', 'else', 'switch', 'select'],
        'keyword.repeat.go': ['for', 'range']
      };
      _results = [];
      for (scope in keywordLists) {
        list = keywordLists[scope];
        _results.push((function() {
          var _i, _len, _results1;
          _results1 = [];
          for (_i = 0, _len = list.length; _i < _len; _i++) {
            keyword = list[_i];
            tokens = grammar.tokenizeLine(keyword).tokens;
            expect(tokens[0].value).toEqual(keyword);
            _results1.push(expect(tokens[0].scopes).toEqual(['source.go', scope]));
          }
          return _results1;
        })());
      }
      return _results;
    });
    it('tokenizes types', function() {
      var tokens, type, types, _i, _len, _results;
      types = ['chan', 'map', 'bool', 'string', 'error', 'int', 'int8', 'int16', 'int32', 'int64', 'rune', 'byte', 'uint', 'uint8', 'uint16', 'uint32', 'uint64', 'uintptr', 'float32', 'float64', 'complex64', 'complex128'];
      _results = [];
      for (_i = 0, _len = types.length; _i < _len; _i++) {
        type = types[_i];
        tokens = grammar.tokenizeLine(type).tokens;
        expect(tokens[0].value).toEqual(type);
        _results.push(expect(tokens[0].scopes).toEqual(['source.go', 'storage.type.go']));
      }
      return _results;
    });
    it('tokenizes "func" as a keyword or type based on context', function() {
      var funcKeyword, funcType, line, next, relevantToken, t, tokens, _i, _j, _len, _len1, _results;
      funcKeyword = ['func f()', 'func (x) f()', 'func(x) f()', 'func'];
      for (_i = 0, _len = funcKeyword.length; _i < _len; _i++) {
        line = funcKeyword[_i];
        tokens = grammar.tokenizeLine(line).tokens;
        expect(tokens[0].value).toEqual('func');
        expect(tokens[0].scopes).toEqual(['source.go', 'keyword.go']);
      }
      funcType = [
        {
          'line': 'var f1 func(',
          'tokenPos': 4
        }, {
          'line': 'f2 :=func()',
          'tokenPos': 3
        }, {
          'line': '\tfunc(',
          'tokenPos': 1
        }, {
          'line': 'type HandlerFunc func(',
          'tokenPos': 4
        }
      ];
      _results = [];
      for (_j = 0, _len1 = funcType.length; _j < _len1; _j++) {
        t = funcType[_j];
        tokens = grammar.tokenizeLine(t.line).tokens;
        relevantToken = tokens[t.tokenPos];
        expect(relevantToken.value).toEqual('func');
        expect(relevantToken.scopes).toEqual(['source.go', 'storage.type.go']);
        next = tokens[t.tokenPos + 1];
        expect(next.value).toEqual('(');
        _results.push(expect(next.scopes).toEqual(['source.go', 'keyword.operator.go']));
      }
      return _results;
    });
    it('tokenizes func names in their declarations', function() {
      var next, relevantToken, t, tests, tokens, _i, _len, _results;
      tests = [
        {
          'line': 'func f()',
          'tokenPos': 2
        }, {
          'line': 'func (T) f()',
          'tokenPos': 2
        }, {
          'line': 'func (t T) f()',
          'tokenPos': 2
        }, {
          'line': 'func (t *T) f()',
          'tokenPos': 2
        }
      ];
      _results = [];
      for (_i = 0, _len = tests.length; _i < _len; _i++) {
        t = tests[_i];
        tokens = grammar.tokenizeLine(t.line).tokens;
        expect(tokens[0].value).toEqual('func');
        expect(tokens[0].scopes).toEqual(['source.go', 'keyword.go']);
        relevantToken = tokens[t.tokenPos];
        expect(relevantToken).toBeDefined();
        expect(relevantToken.value).toEqual('f');
        expect(relevantToken.scopes).toEqual(['source.go', 'support.function.go']);
        next = tokens[t.tokenPos + 1];
        expect(next.value).toEqual('(');
        _results.push(expect(next.scopes).toEqual(['source.go', 'keyword.operator.go']));
      }
      return _results;
    });
    it('tokenizes numerics', function() {
      var invalidOctals, num, numerics, tokens, _i, _j, _len, _len1, _results;
      numerics = ['42', '0600', '0xBadFace', '170141183460469231731687303715884105727', '0.', '72.40', '072.40', '2.71828', '1.e+0', '6.67428e-11', '1E6', '.25', '.12345E+5', '0i', '011i', '0.i', '2.71828i', '1.e+0i', '6.67428e-11i', '1E6i', '.25i', '.12345E+5i'];
      for (_i = 0, _len = numerics.length; _i < _len; _i++) {
        num = numerics[_i];
        tokens = grammar.tokenizeLine(num).tokens;
        expect(tokens[0].value).toEqual(num);
        expect(tokens[0].scopes).toEqual(['source.go', 'constant.numeric.go']);
      }
      invalidOctals = ['08', '039', '0995'];
      _results = [];
      for (_j = 0, _len1 = invalidOctals.length; _j < _len1; _j++) {
        num = invalidOctals[_j];
        tokens = grammar.tokenizeLine(num).tokens;
        expect(tokens[0].value).toEqual(num);
        _results.push(expect(tokens[0].scopes).toEqual(['source.go', 'invalid.illegal.numeric.go']));
      }
      return _results;
    });
    it('tokenizes language constants', function() {
      var constant, constants, tokens, _i, _len, _results;
      constants = ['iota', 'true', 'false', 'nil'];
      _results = [];
      for (_i = 0, _len = constants.length; _i < _len; _i++) {
        constant = constants[_i];
        tokens = grammar.tokenizeLine(constant).tokens;
        expect(tokens[0].value).toEqual(constant);
        _results.push(expect(tokens[0].scopes).toEqual(['source.go', 'constant.language.go']));
      }
      return _results;
    });
    it('tokenizes built-in functions', function() {
      var func, funcs, tokens, _i, _len, _results;
      funcs = ['append', 'cap', 'close', 'complex', 'copy', 'delete', 'imag', 'len', 'make', 'new', 'panic', 'print', 'println', 'real', 'recover'];
      _results = [];
      for (_i = 0, _len = funcs.length; _i < _len; _i++) {
        func = funcs[_i];
        tokens = grammar.tokenizeLine(func).tokens;
        expect(tokens[0].value).toEqual(func);
        _results.push(expect(tokens[0].scopes).toEqual(['source.go', 'support.function.built-in.go']));
      }
      return _results;
    });
    it('tokenizes operators', function() {
      var allKeywords, fullOp, op, opers, scopes, tokens, _i, _len, _results;
      opers = ['+', '&', '+=', '&=', '&&', '==', '!=', '(', ')', '-', '|', '-=', '|=', '||', '<', '<=', '[', ']', '*', '^', '*=', '^=', '<-', '>', '>=', '{', '}', '/', '<<', '/=', '<<=', '++', '=', ':=', ',', ';', '%', '>>', '%=', '>>=', '--', '!', '...', '.', ':', '&^', '&^='];
      _results = [];
      for (_i = 0, _len = opers.length; _i < _len; _i++) {
        op = opers[_i];
        tokens = grammar.tokenizeLine(op).tokens;
        fullOp = tokens.map(function(tok) {
          return tok.value;
        }).join('');
        expect(fullOp).toEqual(op);
        scopes = tokens.map(function(tok) {
          return tok.scopes;
        });
        allKeywords = scopes.every(function(scope) {
          return __indexOf.call(scope, 'keyword.operator.go') >= 0;
        });
        _results.push(expect(allKeywords).toBe(true));
      }
      return _results;
    });
    it('tokenizes func names in calls to them', function() {
      var next, relevantToken, t, tests, tokens, want, _i, _len, _results;
      tests = [
        {
          'line': 'a.b()',
          'name': 'b',
          'tokenPos': 2,
          'isFunc': true
        }, {
          'line': 'pkg.Func1(',
          'name': 'Func1',
          'tokenPos': 2,
          'isFunc': true
        }, {
          'line': 'pkg.Func1().Func2(',
          'name': 'Func2',
          'tokenPos': 6,
          'isFunc': true
        }, {
          'line': 'pkg.var',
          'name': 'var',
          'tokenPos': 2,
          'isFunc': false
        }, {
          'line': 'doWork(ch)',
          'name': 'doWork',
          'tokenPos': 0,
          'isFunc': true
        }, {
          'line': 'f1()',
          'name': 'f1',
          'tokenPos': 0,
          'isFunc': true
        }
      ];
      want = ['source.go', 'support.function.go'];
      _results = [];
      for (_i = 0, _len = tests.length; _i < _len; _i++) {
        t = tests[_i];
        tokens = grammar.tokenizeLine(t.line).tokens;
        relevantToken = tokens[t.tokenPos];
        if (t.isFunc) {
          expect(relevantToken).not.toBeNull();
          expect(relevantToken.value).toEqual(t.name);
          expect(relevantToken.scopes).toEqual(want);
          next = tokens[t.tokenPos + 1];
          expect(next.value).toEqual('(');
          _results.push(expect(next.scopes).toEqual(['source.go', 'keyword.operator.go']));
        } else {
          _results.push(expect(relevantToken.scopes).not.toEqual(want));
        }
      }
      return _results;
    });
    it('tokenizes type names in their declarations', function() {
      var tokens;
      tokens = grammar.tokenizeLine('type Stringer interface {').tokens;
      expect(tokens[0].value).toBe('type');
      expect(tokens[0].scopes).toEqual(['source.go', 'keyword.go']);
      expect(tokens[2].value).toBe('Stringer');
      expect(tokens[2].scopes).toEqual(['source.go', 'storage.type.go']);
      tokens = grammar.tokenizeLine('type Duration int64').tokens;
      expect(tokens[0].value).toBe('type');
      expect(tokens[0].scopes).toEqual(['source.go', 'keyword.go']);
      expect(tokens[2].value).toBe('Duration');
      expect(tokens[2].scopes).toEqual(['source.go', 'storage.type.go']);
      tokens = grammar.tokenizeLine('type   byLength []string').tokens;
      expect(tokens[0].value).toBe('type');
      expect(tokens[0].scopes).toEqual(['source.go', 'keyword.go']);
      expect(tokens[2].value).toBe('byLength');
      expect(tokens[2].scopes).toEqual(['source.go', 'storage.type.go']);
      tokens = grammar.tokenizeLine('  type T').tokens;
      expect(tokens[2].value).toBe(' T');
      return expect(tokens[2].scopes).not.toEqual(['source.go', 'storage.type.go']);
    });
    return describe('in variable declarations', function() {
      var testName, testOp, testVar, wantedScope;
      testVar = function(token) {
        expect(token.value).toBe('var');
        return expect(token.scopes).toEqual(['source.go', 'keyword.go']);
      };
      wantedScope = ['source.go', 'variable.go'];
      testName = function(token, name) {
        expect(token.value).toBe(name);
        return expect(token.scopes).toEqual(wantedScope);
      };
      testOp = function(token, op) {
        expect(token.value).toBe(op);
        return expect(token.scopes).toEqual(['source.go', 'keyword.operator.go']);
      };
      return describe('in "var" statements', function() {
        it('tokenizes single names', function() {
          var tokens;
          tokens = grammar.tokenizeLine('var i int').tokens;
          testVar(tokens[0]);
          testName(tokens[2], 'i');
          tokens = grammar.tokenizeLine(' var k =  0').tokens;
          testVar(tokens[1]);
          return testName(tokens[3], 'k');
        });
        it('tokenizes multiple names', function() {
          var tokens;
          tokens = grammar.tokenizeLine('var U, V,  W  float64').tokens;
          testVar(tokens[0]);
          testName(tokens[2], 'U');
          testOp(tokens[3], ',');
          testName(tokens[5], 'V');
          testName(tokens[8], 'W');
          tokens = grammar.tokenizeLine('var x, y float32 = float, thirtytwo').tokens;
          testVar(tokens[0]);
          testName(tokens[2], 'x');
          testName(tokens[5], 'y');
          expect(tokens[7].value).toBe('float32');
          expect(tokens[7].scopes).toEqual(['source.go', 'storage.type.go']);
          testOp(tokens[9], '=');
          expect(tokens[10].value).toBe(' float');
          expect(tokens[10].scopes).toEqual(['source.go']);
          tokens = grammar.tokenizeLine('var re, im = complexSqrt(-1)').tokens;
          testVar(tokens[0]);
          testName(tokens[2], 're');
          testName(tokens[5], 'im');
          testOp(tokens[7], '=');
          tokens = grammar.tokenizeLine('var _, found = entries[name]').tokens;
          testVar(tokens[0]);
          testName(tokens[2], '_');
          testName(tokens[5], 'found');
          return testOp(tokens[7], '=');
        });
        describe('in "var" statement blocks', function() {
          it('tokenizes single names', function() {
            var decl, kwd, _, _ref;
            _ref = grammar.tokenizeLines('\tvar (\n\t\tfoo *bar\n\t)'), kwd = _ref[0], decl = _ref[1], _ = _ref[2];
            testVar(kwd[1]);
            testOp(kwd[3], '(');
            return testName(decl[1], 'foo');
          });
          return it('tokenizes multiple names', function() {
            var decl, kwd, _, _ref;
            _ref = grammar.tokenizeLines('var (\n\n\tfoo, bar = baz, quux\n)'), kwd = _ref[0], _ = _ref[1], decl = _ref[2], _ = _ref[3];
            testVar(kwd[0]);
            testName(decl[1], 'foo');
            return testName(decl[4], 'bar');
          });
        });
        return describe('in shorthand variable declarations', function() {
          it('tokenizes single names', function() {
            var tokens;
            tokens = grammar.tokenizeLine('f := func() int { return 7 }').tokens;
            testName(tokens[0], 'f');
            testOp(tokens[2], ':=');
            tokens = grammar.tokenizeLine('ch := make(chan int)').tokens;
            testName(tokens[0], 'ch');
            return testOp(tokens[2], ':=');
          });
          return xit('tokenizes multiple names', function() {
            var tokens;
            tokens = grammar.tokenizeLine('i, j := 0, 10').tokens;
            testName(tokens[0], 'i');
            testOp(tokens[1], ',');
            testName(tokens[3], 'j');
            tokens = grammar.tokenizeLine('if _, y, z := coord(p); z > 0').tokens;
            testName(tokens[2], '_');
            testName(tokens[5], 'y');
            testName(tokens[8], 'z');
            return testOp(tokens[10], ':=');
          });
        });
      });
    });
  });

}).call(this);
